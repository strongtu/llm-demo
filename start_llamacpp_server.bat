REM python -m llama_cpp.server --model "..\llama.cpp\models\vicuna-13b-v1.5-16k.Q4_K_M.gguf" --n_ctx 8192 --n_gpu_layers 128
python -m llama_cpp.server --model "..\llama.cpp\models\chinese-alpaca-2-13b-16k.Q4_K.gguf" --n_ctx 8192 --n_gpu_layers 128 
REM python -m llama_cpp.server --model "..\llama.cpp\models\claude2-alpaca-13b.Q4_K_M.gguf" --n_ctx 8192 --n_gpu_layers 128 